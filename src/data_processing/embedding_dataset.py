import torch
from torch.utils.data import Dataset
import json
import os
import pandas as pd
import numpy as np

class MELDEmbeddingDataset(Dataset):
    """
    PyTorch Dataset for loading pre-calculated audio and text embeddings
    generated by create_embeddings.py (BERT-base + WavLM-base-plus).
    Ensures every CSV row is represented (with zero-vector fallback if needed).
    """

    def __init__(self, project_root, data_type='train'):
        """
        Initializes the dataset by locating the processed JSON files and mapping labels.
        """
        self.project_root = project_root
        self.data_type = data_type
        self.json_dir = os.path.join(self.project_root, 'data', 'processed', 'json_embeddings')
        raw_data_dir = os.path.join(self.project_root, 'data', 'raw')

        if self.data_type == 'train':
            csv_path = os.path.join(raw_data_dir, 'train_sent_emo.csv')
        elif self.data_type == 'dev':
            csv_path = os.path.join(raw_data_dir, 'dev_sent_emo.csv')
        else:  # test
            csv_path = os.path.join(raw_data_dir, 'test_sent_emo.csv')

        if not os.path.exists(csv_path):
            raise FileNotFoundError(f"CSV file not found at {csv_path}")

        self.df = pd.read_csv(csv_path)

        # Map labels
        self.sentiment_mapping = {'positive': 0, 'negative': 1, 'neutral': 2}
        self.emotion_mapping = {
            'neutral': 0, 'joy': 1, 'sadness': 2,
            'fear': 3, 'anger': 4, 'surprise': 5, 'disgust': 6
        }

        print(f"Loaded {len(self.df)} rows from {os.path.basename(csv_path)}")
        print(f"Looking for JSONs in {self.json_dir}")

    def __len__(self):
        """Returns the total number of samples in the dataset (same as CSV)."""
        return len(self.df)

    def __getitem__(self, idx):
        """
        Fetches the sample at the given index, loads its data (or creates fallback).
        """
        row = self.df.iloc[idx]
        dialogue_id = row['Dialogue_ID']
        utterance_id = row['Utterance_ID']
        json_filename = f"dia{dialogue_id}_utt{utterance_id}.json"
        json_path = os.path.join(self.json_dir, json_filename)

        if not os.path.exists(json_path):
            print(f"⚠️ WARNING: JSON not found for {json_filename}. Using placeholder embeddings.")
            audio_embedding = np.zeros(768, dtype=np.float32)
            text_embedding = np.zeros(768, dtype=np.float32)
            sentiment = row['Sentiment']
            emotion = row['Emotion']
        else:
            try:
                with open(json_path, 'r') as f:
                    data = json.load(f)

                audio_embedding = np.array(data.get('audio_embedding', []), dtype=np.float32)
                text_embedding = np.array(data.get('text_embedding', []), dtype=np.float32)
                sentiment = data.get('sentiment', row['Sentiment'])
                emotion = data.get('emotion', row['Emotion'])

                # ✅ Dynamic dimension detection with fallback
                if audio_embedding.size == 0:
                    audio_embedding = np.zeros(768, dtype=np.float32)
                if text_embedding.size == 0:
                    text_embedding = np.zeros(768, dtype=np.float32)

                # Detect placeholder vectors
                if not np.any(audio_embedding):
                    print(f"⚠️ Placeholder audio embedding in {json_filename}")
                if not np.any(text_embedding):
                    print(f"⚠️ Placeholder text embedding in {json_filename}")

            except (json.JSONDecodeError, KeyError, TypeError, ValueError) as e:
                print(f"⚠️ ERROR reading {json_filename}. Using placeholders. Error: {e}")
                audio_embedding = np.zeros(768, dtype=np.float32)
                text_embedding = np.zeros(768, dtype=np.float32)
                sentiment = row['Sentiment']
                emotion = row['Emotion']

        # Convert to tensors
        audio_tensor = torch.tensor(audio_embedding, dtype=torch.float32)
        text_tensor = torch.tensor(text_embedding, dtype=torch.float32)

        sample = {
            "audio_embedding": audio_tensor,   # [768]
            "text_embedding": text_tensor,     # [768]
            "features": torch.cat((audio_tensor, text_tensor), dim=0),  # [1536]
            "sentiment_label": torch.tensor(self.sentiment_mapping.get(sentiment, 2), dtype=torch.long),
            "emotion_label": torch.tensor(self.emotion_mapping.get(emotion, 0), dtype=torch.long),
            "dialogue_id": dialogue_id,
            "utterance_id": utterance_id
        }
        return sample
